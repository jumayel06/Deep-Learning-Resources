{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Dataset:*** https://www.kaggle.com/c/si650winter11/data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from string import punctuation\n",
    "from collections import Counter\n",
    "from sklearn.utils import shuffle\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "labeled_data = pd.read_csv('datasets/kaggle/training.txt', sep='\\t')\n",
    "labeled_data.columns =['Class', 'Data']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "unlabeled_data = pd.read_csv('datasets/kaggle/testing.txt', sep='\\t')\n",
    "unlabeled_data.columns = ['Data']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "labeled_data = shuffle(labeled_data)\n",
    "unlabeled_data = shuffle(unlabeled_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Class</th>\n",
       "      <th>Data</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4127</th>\n",
       "      <td>0</td>\n",
       "      <td>Besides, Da Vinci Code sucks...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3685</th>\n",
       "      <td>1</td>\n",
       "      <td>Brokeback Mountain was so awesome.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3070</th>\n",
       "      <td>1</td>\n",
       "      <td>I love playing defensive positions and i love ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1147</th>\n",
       "      <td>1</td>\n",
       "      <td>i like mission impossible.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5808</th>\n",
       "      <td>0</td>\n",
       "      <td>Is it just me, or does Harry Potter suck?...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Class                                               Data\n",
       "4127      0                    Besides, Da Vinci Code sucks...\n",
       "3685      1                 Brokeback Mountain was so awesome.\n",
       "3070      1  I love playing defensive positions and i love ...\n",
       "1147      1                         i like mission impossible.\n",
       "5808      0       Is it just me, or does Harry Potter suck?..."
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labeled_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Data</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>18937</th>\n",
       "      <td>I need to pay Geico and a host of other bills ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12164</th>\n",
       "      <td>I like Tom Cruise, as I've stated over and over.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5716</th>\n",
       "      <td>. I'm pleased to announce that Boston sucked...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1439</th>\n",
       "      <td>If, however, your eyes can't handle the resolu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15957</th>\n",
       "      <td>Tom Cruise sucks.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    Data\n",
       "18937  I need to pay Geico and a host of other bills ...\n",
       "12164   I like Tom Cruise, as I've stated over and over.\n",
       "5716     . I'm pleased to announce that Boston sucked...\n",
       "1439   If, however, your eyes can't handle the resolu...\n",
       "15957                                  Tom Cruise sucks."
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unlabeled_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "labels = labeled_data.iloc[:, 0].values\n",
    "reviews = labeled_data.iloc[:, 1].values\n",
    "unlabeled_reviews = unlabeled_data.iloc[:,0].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 1, ..., 1, 1, 0])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Besides, Da Vinci Code sucks...',\n",
       "       'Brokeback Mountain was so awesome.',\n",
       "       'I love playing defensive positions and i love brokeback mountain..',\n",
       "       ...,\n",
       "       'i love being a sentry for mission impossible and a station for bonkers.',\n",
       "       '* brokeback mountain is an awesome movie..',\n",
       "       'I hate Harry Potter, that daniel wotshisface needs a fucking slap...'], dtype=object)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 'I need to pay Geico and a host of other bills but that is neither here nor there.',\n",
       "       \"I like Tom Cruise, as I've stated over and over.\",\n",
       "       \". I'm pleased to announce that Boston sucked...\", ...,\n",
       "       'Boston can suck my fucking tits...',\n",
       "       'Toyota is doing some amazing things with fuel economy.',\n",
       "       'Stupid UCLA.'], dtype=object)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unlabeled_reviews"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "reviews_processed = []\n",
    "unlabeled_processed = [] \n",
    "for review in reviews:\n",
    "    review_cool_one = ''.join([char for char in review if char not in punctuation])\n",
    "    reviews_processed.append(review_cool_one)\n",
    "    \n",
    "for review in unlabeled_reviews:\n",
    "    review_cool_one = ''.join([char for char in review if char not in punctuation])\n",
    "    unlabeled_processed.append(review_cool_one)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Besides Da Vinci Code sucks',\n",
       " 'Brokeback Mountain was so awesome',\n",
       " 'I love playing defensive positions and i love brokeback mountain',\n",
       " 'i like mission impossible',\n",
       " 'Is it just me or does Harry Potter suck']"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reviews_processed[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "word_reviews = []\n",
    "word_unlabeled = []\n",
    "all_words = []\n",
    "for review in reviews_processed:\n",
    "    word_reviews.append(review.lower().split())\n",
    "    for word in review.split():\n",
    "        all_words.append(word.lower())\n",
    "\n",
    "for review in unlabeled_processed:\n",
    "    word_unlabeled.append(review.lower().split())\n",
    "    for word in review.split():\n",
    "        all_words.append(word.lower())\n",
    "    \n",
    "counter = Counter(all_words)\n",
    "vocab = sorted(counter, key=counter.get, reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['besides', 'da', 'vinci', 'code', 'sucks'],\n",
       " ['brokeback', 'mountain', 'was', 'so', 'awesome'],\n",
       " ['i',\n",
       "  'love',\n",
       "  'playing',\n",
       "  'defensive',\n",
       "  'positions',\n",
       "  'and',\n",
       "  'i',\n",
       "  'love',\n",
       "  'brokeback',\n",
       "  'mountain'],\n",
       " ['i', 'like', 'mission', 'impossible'],\n",
       " ['is', 'it', 'just', 'me', 'or', 'does', 'harry', 'potter', 'suck']]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_reviews[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i', 'the', 'and', 'love', 'is', 'a', 'to', 'my', 'of', 'that']"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Process Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17\n"
     ]
    }
   ],
   "source": [
    "vocab_to_int = {word: i for i, word in enumerate(vocab, 1)}\n",
    "print(vocab_to_int['awesome'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[43, 44, 14, 15, 17]\n"
     ]
    }
   ],
   "source": [
    "reviews_to_ints = []\n",
    "for review in word_reviews:\n",
    "    reviews_to_ints.append([vocab_to_int[word] for word in review])\n",
    "print(reviews_to_ints[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "unlabeled_to_ints = []\n",
    "for review in word_unlabeled:\n",
    "    unlabeled_to_ints.append([vocab_to_int[word] for word in review])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({4: 1111, 5: 770, 6: 716, 7: 591, 11: 533, 15: 446, 8: 399, 14: 286, 12: 278, 21: 253, 9: 212, 25: 170, 10: 157, 13: 130, 16: 115, 17: 104, 20: 97, 22: 94, 28: 88, 34: 84, 33: 83, 3: 35, 19: 33, 18: 30, 23: 19, 24: 14, 27: 13, 31: 11, 26: 10, 29: 7, 32: 6, 30: 5, 36: 4, 72: 2, 35: 2, 311: 1, 63: 1, 104: 1, 931: 1, 38: 1, 333: 1, 175: 1, 44: 1, 40: 1})\n"
     ]
    }
   ],
   "source": [
    "reviews_lens = Counter([len(x) for x in reviews_to_ints])\n",
    "print(reviews_lens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Zero-length 0\n",
      "Max review length 931\n"
     ]
    }
   ],
   "source": [
    "print('Zero-length {}'.format(reviews_lens[0]))\n",
    "print(\"Max review length {}\".format(max(reviews_lens)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6917\n",
      "(6917, 250)\n",
      "[[0 0 0 ..., 0 0 0]\n",
      " [0 0 0 ..., 0 0 0]\n",
      " [0 0 0 ..., 0 0 0]\n",
      " ..., \n",
      " [0 0 0 ..., 0 0 0]\n",
      " [0 0 0 ..., 0 0 0]\n",
      " [0 0 0 ..., 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "# Creating Word Vectors\n",
    "seq_len = 250\n",
    "\n",
    "features = np.zeros((len(reviews_to_ints), seq_len), dtype=int)\n",
    "print(len(reviews_to_ints))\n",
    "print(features.shape)\n",
    "print(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  0   0   0 ...,  41  45  20]\n",
      " [  0   0   0 ...,  14  15  17]\n",
      " [  0   0   0 ...,   4  43  44]\n",
      " ..., \n",
      " [  0   0   0 ..., 675  21 945]\n",
      " [  0   0   0 ...,  76  17 102]\n",
      " [  0   0   0 ...,   6 153 788]]\n"
     ]
    }
   ],
   "source": [
    "for i, review in enumerate(reviews_to_ints):\n",
    "    features[i, -len(review):] = np.array(review)[:seq_len]\n",
    "print(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28936\n",
      "(28936, 250)\n",
      "[[   0    0    0 ...,  114  682  113]\n",
      " [   0    0    0 ...,  129    3  129]\n",
      " [   0    0    0 ...,   10   35   81]\n",
      " ..., \n",
      " [   0    0    0 ...,    8  153 1080]\n",
      " [   0    0    0 ...,   52  624  701]\n",
      " [   0    0    0 ...,    0   19   42]]\n"
     ]
    }
   ],
   "source": [
    "features_test = np.zeros((len(unlabeled_to_ints), seq_len), dtype=int)\n",
    "for i, review in enumerate(unlabeled_to_ints):\n",
    "    features_test[i, -len(review):] = np.array(review)[:seq_len]\n",
    "print(len(unlabeled_to_ints))\n",
    "print(features_test.shape)\n",
    "print(features_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape (6400, 250)\n",
      "X_test shape (517, 250)\n",
      "X_unlabeled shape (28936, 250)\n"
     ]
    }
   ],
   "source": [
    "X_train = features[:6400]\n",
    "y_train = labels[:6400]\n",
    "\n",
    "X_test = features[6400:]\n",
    "y_test = labels[6400:]\n",
    "\n",
    "X_unlabeled = features_test\n",
    "\n",
    "print('X_train shape {}'.format(X_train.shape))\n",
    "print('X_test shape {}'.format(X_test.shape))\n",
    "print('X_unlabeled shape {}'.format(X_unlabeled.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build RNN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "hidden_layer_size = 512\n",
    "number_of_layers = 1 \n",
    "batch_size = 100\n",
    "learning_rate = 0.001\n",
    "number_of_words = len(vocab_to_int) + 1\n",
    "dropout_rate = 0.8 \n",
    "embedding_size = 300 \n",
    "epochs = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Placeholders\n",
    "inputs = tf.placeholder(tf.int32, [None, None], name='inputs')\n",
    "targets = tf.placeholder(tf.int32, [None, None], name='targets')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Embedding Layer\n",
    "word_embedings = tf.Variable(tf.random_uniform((number_of_words, embedding_size), -1, 1))\n",
    "embed = tf.nn.embedding_lookup(word_embedings, inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Hidden Layer\n",
    "hidden_layer = tf.contrib.rnn.BasicLSTMCell(hidden_layer_size)\n",
    "hidden_layer = tf.contrib.rnn.DropoutWrapper(hidden_layer, dropout_rate)\n",
    "\n",
    "cell = tf.contrib.rnn.MultiRNNCell([hidden_layer]*number_of_layers)\n",
    "init_state = cell.zero_state(batch_size, tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "outputs, states = tf.nn.dynamic_rnn(cell, embed, initial_state=init_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Backpropagation\n",
    "prediction = tf.layers.dense(outputs[:, -1], 1, activation=tf.sigmoid)\n",
    "cost = tf.losses.mean_squared_error(targets, prediction)\n",
    "\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate).minimize(cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Accuracy\n",
    "currect_pred = tf.equal(tf.cast(tf.round(prediction), tf.int32), targets)\n",
    "accuracy = tf.reduce_mean(tf.cast(currect_pred, tf.float32))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/3  | Current loss: 0.04499276727437973  | Training accuracy: 94.4375\n",
      "Epoch: 2/3  | Current loss: 0.008344389498233795  | Training accuracy: 98.9531\n",
      "Epoch: 3/3  | Current loss: 0.005247828084975481  | Training accuracy: 99.3906\n"
     ]
    }
   ],
   "source": [
    "session = tf.Session()\n",
    "session.run(tf.global_variables_initializer())\n",
    "for i in range(epochs):\n",
    "    training_accurcy = []\n",
    "    ii = 0\n",
    "    epoch_loss = []\n",
    "    while ii + batch_size <= len(X_train):\n",
    "        X_batch = X_train[ii:ii+batch_size]\n",
    "        y_batch = y_train[ii:ii+batch_size].reshape(-1, 1)\n",
    "        \n",
    "        a, o, _ = session.run([accuracy, cost, optimizer], feed_dict={inputs:X_batch, targets:y_batch})\n",
    "\n",
    "        training_accurcy.append(a)\n",
    "        epoch_loss.append(o)\n",
    "        ii += batch_size\n",
    "    print('Epoch: {}/{}'.format(i + 1, epochs), ' | Current loss: {}'.format(np.mean(epoch_loss)),\n",
    "          ' | Training accuracy: {:.4f}'.format(np.mean(training_accurcy)*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy is 98.8000%\n"
     ]
    }
   ],
   "source": [
    "test_accuracy = []\n",
    "\n",
    "ii = 0\n",
    "while ii + batch_size <= len(X_test):\n",
    "    X_batch = X_test[ii:ii+batch_size]\n",
    "    y_batch = y_test[ii:ii+batch_size].reshape(-1, 1)\n",
    "\n",
    "    a = session.run([accuracy], feed_dict={inputs:X_batch, targets:y_batch})\n",
    "    \n",
    "    test_accuracy.append(a)\n",
    "    ii += batch_size\n",
    "print(\"Test accuracy is {:.4f}%\".format(np.mean(test_accuracy)*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "predictions_unlabeled = []\n",
    "ii = 0\n",
    "while ii + batch_size <= len(X_unlabeled):\n",
    "    if ii + batch_size > len(X_unlabeled):\n",
    "        batch_size = len(X_unlabeled) - ii\n",
    "    X_batch = X_unlabeled[ii:ii+batch_size]\n",
    "    y_batch = X_unlabeled[ii:ii+batch_size].reshape(-1, 1)\n",
    "\n",
    "    pred = session.run([prediction], feed_dict={inputs:X_batch, targets:y_batch})\n",
    "    \n",
    "    predictions_unlabeled.append(pred)\n",
    "    ii += batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_real  = []\n",
    "for i in range(len(predictions_unlabeled)):\n",
    "    for ii in range(len(predictions_unlabeled[i][0])):\n",
    "        if predictions_unlabeled[i][0][ii][0] >= 0.5:\n",
    "            pred_real.append(1)\n",
    "        else:\n",
    "            pred_real.append(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.savetxt('predictions.txt', pred_real)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_dataframe = unlabeled_data[:len(pred_real)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "new_dataframe['Classes'] = pred_real"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Data</th>\n",
       "      <th>Classes</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>18937</th>\n",
       "      <td>I need to pay Geico and a host of other bills ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12164</th>\n",
       "      <td>I like Tom Cruise, as I've stated over and over.</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5716</th>\n",
       "      <td>. I'm pleased to announce that Boston sucked...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1439</th>\n",
       "      <td>If, however, your eyes can't handle the resolu...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15957</th>\n",
       "      <td>Tom Cruise sucks.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23678</th>\n",
       "      <td>By the time I left the Hospital, I had no time...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26653</th>\n",
       "      <td>Three days at Purdue with three awesome people.</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24986</th>\n",
       "      <td>I really, really hate TOM CRUISE.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5761</th>\n",
       "      <td>I really hate Tom Cruise.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20284</th>\n",
       "      <td>How terrible London is ar ~ ~ share a bit with...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11384</th>\n",
       "      <td>I want a 2006 black honda interceptor..</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28707</th>\n",
       "      <td>I like the GEICO commercials, too..</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18794</th>\n",
       "      <td>Previously, I have installed Windoze just so I...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6510</th>\n",
       "      <td>I know you're way too smart and way too cool t...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25834</th>\n",
       "      <td>Then we had stupid trivia about San Francisco ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14179</th>\n",
       "      <td>stupid lakers.....</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16975</th>\n",
       "      <td>GEICO can go suck a french fry. They're sorta ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28059</th>\n",
       "      <td>My State Farm time was a very bitter and unple...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13129</th>\n",
       "      <td>I mean, we knew Harvard was dumb anyway -- rig...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15977</th>\n",
       "      <td>UCLA sucks anyways.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2145</th>\n",
       "      <td>PURDUE FUCKIN ROCKS!!!</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27577</th>\n",
       "      <td>Oh, how I onced loved to make Harvard undergra...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7966</th>\n",
       "      <td>The citizens and government of San Francisco a...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23341</th>\n",
       "      <td>i miss boston!!</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13132</th>\n",
       "      <td>我還在用that stupid BOX CAR-HONDA CRX = o = 。 CRAP...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18726</th>\n",
       "      <td>Then we had stupid trivia about San Francisco ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8518</th>\n",
       "      <td>I liked Tom Cruise until he dumped Nicole Kidman.</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23110</th>\n",
       "      <td>I miss crossing London's bridges ( and am suff...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19249</th>\n",
       "      <td>Well apparently all people driving toyota 4run...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2446</th>\n",
       "      <td>im going to give the tahoe to either my mom or...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>499</th>\n",
       "      <td>A guy in my English class has a sister who wil...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22255</th>\n",
       "      <td>And Tom Cruise is beautiful....</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11457</th>\n",
       "      <td>I need an ATA case..</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18454</th>\n",
       "      <td>and i LOVE tom cruise too.</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21625</th>\n",
       "      <td>i love seattle..</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26631</th>\n",
       "      <td>Yeah Geico sucks and they don't save you 15 pe...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14889</th>\n",
       "      <td>I really hate Tom Cruise.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13903</th>\n",
       "      <td>So I guess I still love UCLA.</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18878</th>\n",
       "      <td>stupid, stupid UCLA..</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9859</th>\n",
       "      <td>I hate Tom Cruise..</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24993</th>\n",
       "      <td>I think it's pretty clear Angelina Jolie is an...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20021</th>\n",
       "      <td>Boston's been awesome.</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7265</th>\n",
       "      <td>I love my friends at Purdue.</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7464</th>\n",
       "      <td>the fact that i love paris hilton or like the ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10148</th>\n",
       "      <td>On a lighter note, I love my macbook.</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7195</th>\n",
       "      <td>It's not ideal-a nice tenure-track job would h...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25317</th>\n",
       "      <td>you can say the people at berkeley suck but i ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22156</th>\n",
       "      <td>I needed San Francisco but not until a month ago.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5149</th>\n",
       "      <td>San Francisco's Disastrous Policy Exposed!..</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15712</th>\n",
       "      <td>As i stepped out of my beautiful Toyota, i hea...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28541</th>\n",
       "      <td>I love the Los Angeles LAKERS!!!.</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27726</th>\n",
       "      <td>i hate the lakers..</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14101</th>\n",
       "      <td>the fact that i love paris hilton or like the ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27185</th>\n",
       "      <td>However you look at it, I've been here almost ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28609</th>\n",
       "      <td>Personally, I blame Angelina Jolie, a psycholo...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8852</th>\n",
       "      <td>i try to enjoy my time here in san francisco.....</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9512</th>\n",
       "      <td>I LOVE SEATTLE.</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25347</th>\n",
       "      <td>I love Angelina Jolie.</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18275</th>\n",
       "      <td>I LOVE SEATTLE.</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8889</th>\n",
       "      <td>I got 1 of my 3 licenses i need for State Farm.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>28900 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    Data  Classes\n",
       "18937  I need to pay Geico and a host of other bills ...        0\n",
       "12164   I like Tom Cruise, as I've stated over and over.        1\n",
       "5716     . I'm pleased to announce that Boston sucked...        0\n",
       "1439   If, however, your eyes can't handle the resolu...        0\n",
       "15957                                  Tom Cruise sucks.        0\n",
       "23678  By the time I left the Hospital, I had no time...        0\n",
       "26653    Three days at Purdue with three awesome people.        1\n",
       "24986                  I really, really hate TOM CRUISE.        0\n",
       "5761                           I really hate Tom Cruise.        0\n",
       "20284  How terrible London is ar ~ ~ share a bit with...        0\n",
       "11384            I want a 2006 black honda interceptor..        0\n",
       "28707                I like the GEICO commercials, too..        1\n",
       "18794  Previously, I have installed Windoze just so I...        1\n",
       "6510   I know you're way too smart and way too cool t...        0\n",
       "25834  Then we had stupid trivia about San Francisco ...        0\n",
       "14179                                 stupid lakers.....        0\n",
       "16975  GEICO can go suck a french fry. They're sorta ...        0\n",
       "28059  My State Farm time was a very bitter and unple...        1\n",
       "13129  I mean, we knew Harvard was dumb anyway -- rig...        1\n",
       "15977                                UCLA sucks anyways.        0\n",
       "2145                              PURDUE FUCKIN ROCKS!!!        1\n",
       "27577  Oh, how I onced loved to make Harvard undergra...        1\n",
       "7966   The citizens and government of San Francisco a...        0\n",
       "23341                                    i miss boston!!        1\n",
       "13132  我還在用that stupid BOX CAR-HONDA CRX = o = 。 CRAP...        0\n",
       "18726  Then we had stupid trivia about San Francisco ...        0\n",
       "8518   I liked Tom Cruise until he dumped Nicole Kidman.        1\n",
       "23110  I miss crossing London's bridges ( and am suff...        0\n",
       "19249  Well apparently all people driving toyota 4run...        1\n",
       "2446   im going to give the tahoe to either my mom or...        0\n",
       "...                                                  ...      ...\n",
       "499    A guy in my English class has a sister who wil...        0\n",
       "22255                    And Tom Cruise is beautiful....        1\n",
       "11457                               I need an ATA case..        1\n",
       "18454                         and i LOVE tom cruise too.        1\n",
       "21625                                   i love seattle..        1\n",
       "26631  Yeah Geico sucks and they don't save you 15 pe...        0\n",
       "14889                          I really hate Tom Cruise.        0\n",
       "13903                      So I guess I still love UCLA.        1\n",
       "18878                              stupid, stupid UCLA..        0\n",
       "9859                                 I hate Tom Cruise..        0\n",
       "24993  I think it's pretty clear Angelina Jolie is an...        0\n",
       "20021                             Boston's been awesome.        1\n",
       "7265                        I love my friends at Purdue.        1\n",
       "7464   the fact that i love paris hilton or like the ...        1\n",
       "10148              On a lighter note, I love my macbook.        1\n",
       "7195   It's not ideal-a nice tenure-track job would h...        1\n",
       "25317  you can say the people at berkeley suck but i ...        0\n",
       "22156  I needed San Francisco but not until a month ago.        0\n",
       "5149        San Francisco's Disastrous Policy Exposed!..        0\n",
       "15712  As i stepped out of my beautiful Toyota, i hea...        1\n",
       "28541                  I love the Los Angeles LAKERS!!!.        1\n",
       "27726                                i hate the lakers..        0\n",
       "14101  the fact that i love paris hilton or like the ...        1\n",
       "27185  However you look at it, I've been here almost ...        0\n",
       "28609  Personally, I blame Angelina Jolie, a psycholo...        1\n",
       "8852   i try to enjoy my time here in san francisco.....        1\n",
       "9512                                     I LOVE SEATTLE.        1\n",
       "25347                             I love Angelina Jolie.        1\n",
       "18275                                    I LOVE SEATTLE.        1\n",
       "8889     I got 1 of my 3 licenses i need for State Farm.        0\n",
       "\n",
       "[28900 rows x 2 columns]"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_dataframe"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
