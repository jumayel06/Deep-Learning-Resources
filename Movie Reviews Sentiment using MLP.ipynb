{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Dataset:*** http://ai.stanford.edu/~amaas/data/sentiment/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load the IMDB Dataset With Keras\n",
    "Keras provides access to the IMDB dataset built-in. The ***keras.datasets.imdb.load_data()*** allows you to load the dataset in a format that is ready for use in neural network and deep learning models. Calling imdb.load_data() the first time will download the IMDB dataset to your computer and store it in your home directory under ~/.keras/datasets/imdb.pkl as a 32 megabyte file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy\n",
    "from keras.datasets import imdb\n",
    "from matplotlib import pyplot\n",
    "from keras.preprocessing import sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://s3.amazonaws.com/text-datasets/imdb.npz\n",
      "17465344/17464789 [==============================] - 33s 2us/step\n"
     ]
    }
   ],
   "source": [
    "# load the dataset\n",
    "(X_train, y_train), (X_test, y_test) = imdb.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(25000,) (25000,) (25000,) (25000,)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape, y_train.shape, X_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = numpy.concatenate((X_train, X_test), axis=0)\n",
    "y = numpy.concatenate((y_train, y_test), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data: \n",
      "(50000,) (50000,)\n"
     ]
    }
   ],
   "source": [
    "# summarize size\n",
    "print(\"Training data: \")\n",
    "print(X.shape, y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1,\n",
       " 14,\n",
       " 22,\n",
       " 16,\n",
       " 43,\n",
       " 530,\n",
       " 973,\n",
       " 1622,\n",
       " 1385,\n",
       " 65,\n",
       " 458,\n",
       " 4468,\n",
       " 66,\n",
       " 3941,\n",
       " 4,\n",
       " 173,\n",
       " 36,\n",
       " 256,\n",
       " 5,\n",
       " 25,\n",
       " 100,\n",
       " 43,\n",
       " 838,\n",
       " 112,\n",
       " 50,\n",
       " 670,\n",
       " 22665,\n",
       " 9,\n",
       " 35,\n",
       " 480,\n",
       " 284,\n",
       " 5,\n",
       " 150,\n",
       " 4,\n",
       " 172,\n",
       " 112,\n",
       " 167,\n",
       " 21631,\n",
       " 336,\n",
       " 385,\n",
       " 39,\n",
       " 4,\n",
       " 172,\n",
       " 4536,\n",
       " 1111,\n",
       " 17,\n",
       " 546,\n",
       " 38,\n",
       " 13,\n",
       " 447,\n",
       " 4,\n",
       " 192,\n",
       " 50,\n",
       " 16,\n",
       " 6,\n",
       " 147,\n",
       " 2025,\n",
       " 19,\n",
       " 14,\n",
       " 22,\n",
       " 4,\n",
       " 1920,\n",
       " 4613,\n",
       " 469,\n",
       " 4,\n",
       " 22,\n",
       " 71,\n",
       " 87,\n",
       " 12,\n",
       " 16,\n",
       " 43,\n",
       " 530,\n",
       " 38,\n",
       " 76,\n",
       " 15,\n",
       " 13,\n",
       " 1247,\n",
       " 4,\n",
       " 22,\n",
       " 17,\n",
       " 515,\n",
       " 17,\n",
       " 12,\n",
       " 16,\n",
       " 626,\n",
       " 18,\n",
       " 19193,\n",
       " 5,\n",
       " 62,\n",
       " 386,\n",
       " 12,\n",
       " 8,\n",
       " 316,\n",
       " 8,\n",
       " 106,\n",
       " 5,\n",
       " 4,\n",
       " 2223,\n",
       " 5244,\n",
       " 16,\n",
       " 480,\n",
       " 66,\n",
       " 3785,\n",
       " 33,\n",
       " 4,\n",
       " 130,\n",
       " 12,\n",
       " 16,\n",
       " 38,\n",
       " 619,\n",
       " 5,\n",
       " 25,\n",
       " 124,\n",
       " 51,\n",
       " 36,\n",
       " 135,\n",
       " 48,\n",
       " 25,\n",
       " 1415,\n",
       " 33,\n",
       " 6,\n",
       " 22,\n",
       " 12,\n",
       " 215,\n",
       " 28,\n",
       " 77,\n",
       " 52,\n",
       " 5,\n",
       " 14,\n",
       " 407,\n",
       " 16,\n",
       " 82,\n",
       " 10311,\n",
       " 8,\n",
       " 4,\n",
       " 107,\n",
       " 117,\n",
       " 5952,\n",
       " 15,\n",
       " 256,\n",
       " 4,\n",
       " 31050,\n",
       " 7,\n",
       " 3766,\n",
       " 5,\n",
       " 723,\n",
       " 36,\n",
       " 71,\n",
       " 43,\n",
       " 530,\n",
       " 476,\n",
       " 26,\n",
       " 400,\n",
       " 317,\n",
       " 46,\n",
       " 7,\n",
       " 4,\n",
       " 12118,\n",
       " 1029,\n",
       " 13,\n",
       " 104,\n",
       " 88,\n",
       " 4,\n",
       " 381,\n",
       " 15,\n",
       " 297,\n",
       " 98,\n",
       " 32,\n",
       " 2071,\n",
       " 56,\n",
       " 26,\n",
       " 141,\n",
       " 6,\n",
       " 194,\n",
       " 7486,\n",
       " 18,\n",
       " 4,\n",
       " 226,\n",
       " 22,\n",
       " 21,\n",
       " 134,\n",
       " 476,\n",
       " 26,\n",
       " 480,\n",
       " 5,\n",
       " 144,\n",
       " 30,\n",
       " 5535,\n",
       " 18,\n",
       " 51,\n",
       " 36,\n",
       " 28,\n",
       " 224,\n",
       " 92,\n",
       " 25,\n",
       " 104,\n",
       " 4,\n",
       " 226,\n",
       " 65,\n",
       " 16,\n",
       " 38,\n",
       " 1334,\n",
       " 88,\n",
       " 12,\n",
       " 16,\n",
       " 283,\n",
       " 5,\n",
       " 16,\n",
       " 4472,\n",
       " 113,\n",
       " 103,\n",
       " 32,\n",
       " 15,\n",
       " 16,\n",
       " 5345,\n",
       " 19,\n",
       " 178,\n",
       " 32]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classes: \n",
      "[0 1]\n"
     ]
    }
   ],
   "source": [
    "# Summarize number of classes\n",
    "print(\"Classes: \")\n",
    "print(numpy.unique(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of words: \n",
      "88585\n"
     ]
    }
   ],
   "source": [
    "# Summarize number of words\n",
    "print(\"Number of words: \")\n",
    "print(len(numpy.unique(numpy.hstack(X))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Review length: \n",
      "Mean 234.76 words (172.911495)\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYAAAAD8CAYAAAB+UHOxAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x181b51b3c8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Summarize review length\n",
    "print(\"Review length: \")\n",
    "result = [len(x) for x in X]\n",
    "print(\"Mean %.2f words (%f)\" % (numpy.mean(result), numpy.std(result)))\n",
    "# plot review length\n",
    "pyplot.boxplot(result)\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking a box and whisker plot for the review lengths in words, we can probably see an exponential distribution that we can probably cover the mass of the distribution with a clipped length of 400 to 500 words.\n",
    "\n",
    "We are going to use Word Embedding which is a technique where words are encoded as real-valued vectors in a high-dimensional space, where the similarity between words in terms of meaning translates to closeness in the vector space.\n",
    "\n",
    "Keras provides a convenient way to convert positive integer representations of words into a word embedding by an Embedding layer.\n",
    "\n",
    "The layer takes arguments that define the mapping including the maximum number of expected words also called the vocabulary size (e.g. the largest integer value that will be seen as an integer). The layer also allows you to specify the dimensionality for each word vector, called the output dimension."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "(X_train, y_train), (X_test, y_test) = imdb.load_data(num_words=5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(25000,)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1,\n",
       " 14,\n",
       " 22,\n",
       " 16,\n",
       " 43,\n",
       " 530,\n",
       " 973,\n",
       " 1622,\n",
       " 1385,\n",
       " 65,\n",
       " 458,\n",
       " 4468,\n",
       " 66,\n",
       " 3941,\n",
       " 4,\n",
       " 173,\n",
       " 36,\n",
       " 256,\n",
       " 5,\n",
       " 25,\n",
       " 100,\n",
       " 43,\n",
       " 838,\n",
       " 112,\n",
       " 50,\n",
       " 670,\n",
       " 2,\n",
       " 9,\n",
       " 35,\n",
       " 480,\n",
       " 284,\n",
       " 5,\n",
       " 150,\n",
       " 4,\n",
       " 172,\n",
       " 112,\n",
       " 167,\n",
       " 2,\n",
       " 336,\n",
       " 385,\n",
       " 39,\n",
       " 4,\n",
       " 172,\n",
       " 4536,\n",
       " 1111,\n",
       " 17,\n",
       " 546,\n",
       " 38,\n",
       " 13,\n",
       " 447,\n",
       " 4,\n",
       " 192,\n",
       " 50,\n",
       " 16,\n",
       " 6,\n",
       " 147,\n",
       " 2025,\n",
       " 19,\n",
       " 14,\n",
       " 22,\n",
       " 4,\n",
       " 1920,\n",
       " 4613,\n",
       " 469,\n",
       " 4,\n",
       " 22,\n",
       " 71,\n",
       " 87,\n",
       " 12,\n",
       " 16,\n",
       " 43,\n",
       " 530,\n",
       " 38,\n",
       " 76,\n",
       " 15,\n",
       " 13,\n",
       " 1247,\n",
       " 4,\n",
       " 22,\n",
       " 17,\n",
       " 515,\n",
       " 17,\n",
       " 12,\n",
       " 16,\n",
       " 626,\n",
       " 18,\n",
       " 2,\n",
       " 5,\n",
       " 62,\n",
       " 386,\n",
       " 12,\n",
       " 8,\n",
       " 316,\n",
       " 8,\n",
       " 106,\n",
       " 5,\n",
       " 4,\n",
       " 2223,\n",
       " 2,\n",
       " 16,\n",
       " 480,\n",
       " 66,\n",
       " 3785,\n",
       " 33,\n",
       " 4,\n",
       " 130,\n",
       " 12,\n",
       " 16,\n",
       " 38,\n",
       " 619,\n",
       " 5,\n",
       " 25,\n",
       " 124,\n",
       " 51,\n",
       " 36,\n",
       " 135,\n",
       " 48,\n",
       " 25,\n",
       " 1415,\n",
       " 33,\n",
       " 6,\n",
       " 22,\n",
       " 12,\n",
       " 215,\n",
       " 28,\n",
       " 77,\n",
       " 52,\n",
       " 5,\n",
       " 14,\n",
       " 407,\n",
       " 16,\n",
       " 82,\n",
       " 2,\n",
       " 8,\n",
       " 4,\n",
       " 107,\n",
       " 117,\n",
       " 2,\n",
       " 15,\n",
       " 256,\n",
       " 4,\n",
       " 2,\n",
       " 7,\n",
       " 3766,\n",
       " 5,\n",
       " 723,\n",
       " 36,\n",
       " 71,\n",
       " 43,\n",
       " 530,\n",
       " 476,\n",
       " 26,\n",
       " 400,\n",
       " 317,\n",
       " 46,\n",
       " 7,\n",
       " 4,\n",
       " 2,\n",
       " 1029,\n",
       " 13,\n",
       " 104,\n",
       " 88,\n",
       " 4,\n",
       " 381,\n",
       " 15,\n",
       " 297,\n",
       " 98,\n",
       " 32,\n",
       " 2071,\n",
       " 56,\n",
       " 26,\n",
       " 141,\n",
       " 6,\n",
       " 194,\n",
       " 2,\n",
       " 18,\n",
       " 4,\n",
       " 226,\n",
       " 22,\n",
       " 21,\n",
       " 134,\n",
       " 476,\n",
       " 26,\n",
       " 480,\n",
       " 5,\n",
       " 144,\n",
       " 30,\n",
       " 2,\n",
       " 18,\n",
       " 51,\n",
       " 36,\n",
       " 28,\n",
       " 224,\n",
       " 92,\n",
       " 25,\n",
       " 104,\n",
       " 4,\n",
       " 226,\n",
       " 65,\n",
       " 16,\n",
       " 38,\n",
       " 1334,\n",
       " 88,\n",
       " 12,\n",
       " 16,\n",
       " 283,\n",
       " 5,\n",
       " 16,\n",
       " 4472,\n",
       " 113,\n",
       " 103,\n",
       " 32,\n",
       " 15,\n",
       " 16,\n",
       " 2,\n",
       " 19,\n",
       " 178,\n",
       " 32]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = sequence.pad_sequences(X_train, maxlen=500)\n",
    "X_test = sequence.pad_sequences(X_test, maxlen=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    1,   14,   22,   16,\n",
       "         43,  530,  973, 1622, 1385,   65,  458, 4468,   66, 3941,    4,\n",
       "        173,   36,  256,    5,   25,  100,   43,  838,  112,   50,  670,\n",
       "          2,    9,   35,  480,  284,    5,  150,    4,  172,  112,  167,\n",
       "          2,  336,  385,   39,    4,  172, 4536, 1111,   17,  546,   38,\n",
       "         13,  447,    4,  192,   50,   16,    6,  147, 2025,   19,   14,\n",
       "         22,    4, 1920, 4613,  469,    4,   22,   71,   87,   12,   16,\n",
       "         43,  530,   38,   76,   15,   13, 1247,    4,   22,   17,  515,\n",
       "         17,   12,   16,  626,   18,    2,    5,   62,  386,   12,    8,\n",
       "        316,    8,  106,    5,    4, 2223,    2,   16,  480,   66, 3785,\n",
       "         33,    4,  130,   12,   16,   38,  619,    5,   25,  124,   51,\n",
       "         36,  135,   48,   25, 1415,   33,    6,   22,   12,  215,   28,\n",
       "         77,   52,    5,   14,  407,   16,   82,    2,    8,    4,  107,\n",
       "        117,    2,   15,  256,    4,    2,    7, 3766,    5,  723,   36,\n",
       "         71,   43,  530,  476,   26,  400,  317,   46,    7,    4,    2,\n",
       "       1029,   13,  104,   88,    4,  381,   15,  297,   98,   32, 2071,\n",
       "         56,   26,  141,    6,  194,    2,   18,    4,  226,   22,   21,\n",
       "        134,  476,   26,  480,    5,  144,   30,    2,   18,   51,   36,\n",
       "         28,  224,   92,   25,  104,    4,  226,   65,   16,   38, 1334,\n",
       "         88,   12,   16,  283,    5,   16, 4472,  113,  103,   32,   15,\n",
       "         16,    2,   19,  178,   32], dtype=int32)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simple Multi-Layer Perceptron Model for the IMDB Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# MLP for the IMDB problem\n",
    "import numpy\n",
    "from keras.datasets import imdb\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Flatten\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.preprocessing import sequence\n",
    "# fix random seed for reproducibility\n",
    "seed = 7\n",
    "numpy.random.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# load the dataset but only keep the top n words, zero the rest\n",
    "top_words = 5000\n",
    "(X_train, y_train), (X_test, y_test) = imdb.load_data(num_words=top_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "max_words = 500\n",
    "X_train = sequence.pad_sequences(X_train, maxlen=max_words)\n",
    "X_test = sequence.pad_sequences(X_test, maxlen=max_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can create our model. We will use an Embedding layer as the input layer, setting the vocabulary to 5,000, the word vector size to 32 dimensions and the input_length to 500. The output of this first layer will be a 32Ã—500 sized matrix.\n",
    "\n",
    "We will flatten the Embedded layers output to one dimension, then use one dense hidden layer of 250 units with a rectifier activation function. The output layer has one neuron and will use a sigmoid activation to output values of 0 and 1 as predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 500, 32)           160000    \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 16000)             0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 250)               4000250   \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 1)                 251       \n",
      "=================================================================\n",
      "Total params: 4,160,501\n",
      "Trainable params: 4,160,501\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# create the model\n",
    "model = Sequential()\n",
    "model.add(Embedding(top_words, 32, input_length=max_words))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(250, activation='relu'))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 25000 samples, validate on 25000 samples\n",
      "Epoch 1/2\n",
      " - 18s - loss: 0.5089 - acc: 0.7123 - val_loss: 0.3180 - val_acc: 0.8620\n",
      "Epoch 2/2\n",
      " - 18s - loss: 0.1904 - acc: 0.9274 - val_loss: 0.3002 - val_acc: 0.8728\n",
      "Accuracy: 87.28%\n"
     ]
    }
   ],
   "source": [
    "# Fit the model\n",
    "model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=2, batch_size=128, verbose=2)\n",
    "# Final evaluation of the model\n",
    "scores = model.evaluate(X_test, y_test, verbose=0)\n",
    "print(\"Accuracy: %.2f%%\" % (scores[1]*100))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
